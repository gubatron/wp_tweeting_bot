#!/usr/bin/python3
import os
import configparser
import feedparser
import urllib.request
import re
import time
import sys
import ssl
import threading
import random
from twitter import Twitter, OAuth, TwitterHTTPError
from bs4 import BeautifulSoup

TWITTER_CONSIDERED_URL_LENGHT = 23
TWEET_MAX_LENGHT = 280
MAX_TITLE_LENGTH = 231
og_image_pattern = re.compile('.*og:image" content="(.*?)".*?\/>')

class Feed(object):
    """
    Base class for feeds.
    Contains all the logic to do authentication on twitter service.
    Each class that inherits must implements process_tweets on his own.
    dry_run method is optional. Just needed to do testing.
    """
    def __init__(self, config, section):
        self.oauth_token = config.get(section, 'oauth_token')
        self.oauth_token_secret = config.get(section, 'oauth_token_secret')
        self.oauth_consumer_key = config.get(section, 'oauth_consumer_key')
        self.oauth_consumer_secret = config.get(section, 'oauth_consumer_secret')
        self.dry_run = config.getboolean(section, 'dry_run')
        self.intervals_between_tweets_in_seconds = config.getint(section, 'intervals_between_tweets_in_seconds')
        self.max_days_in_rotation = -1
        self.max_seconds_in_rotation = -1

        if config.has_option(section, 'max_days_in_rotation'):
            self.max_days_in_rotation = config.getint(section, 'max_days_in_rotation')
        if self.max_days_in_rotation != -1:
            self.max_seconds_in_rotation = self.max_days_in_rotation * 24 * 60 * 60

        # Auth Credentials
        self.oauth = OAuth(self.oauth_token, self.oauth_token_secret, self.oauth_consumer_key, self.oauth_consumer_secret)
        self.twitter_api = Twitter(auth=self.oauth)
        self.twitter_upload_api = Twitter(auth=self.oauth, domain='upload.twitter.com')

    def process_tweets(self):
        raise NotImplementedError

    def __str__(self):
        return str(self.__dict__)

class FileFeed(Feed):
    """
    A file feed reads one tweet per line from a text file, tweets are rotated according to the configuration settings.

    Special configuration options for FileFeed:
       'feed_path': full file path of text file containing tweets
       'repetition_interval_in_seconds': how many seconds to wait before repeating a tweet

    Internally tweets are kept as a list of dictionaries
    [{'msg': <the tweet text>,
      'first_pub_ts': <timestamp of the first time this message was tweeted>,
      'last_pub_ts': <timestamp of the last time this message was tweeted> }]
    """
    def __init__(self, config, section):
        super(FileFeed, self).__init__(config, section)
        self.feed_path = config.get(section, 'feed_path')
        self.shuffle = config.getboolean(section, 'shuffle')
        self.tweets = []
        self.file_feed_last_modified = os.stat(self.feed_path).st_mtime
        self.repetition_interval_in_seconds = config.getint(section, 'repetition_interval_in_seconds')

    def remove_tweet_by_text(self, old_tweet):
        for t in self.tweets:
            if t['msg'] == old_tweet['msg']:
                self.tweets.remove(t)
                break

    def update_tweets(self):
        old_tweets = self.tweets[:]
        new_tweets = FileFeed.load_tweets(self.feed_path, self.shuffle)

        # Delete old tweets that are no longer in the file
        old_to_delete = elems_missing_in_other_list(old_tweets, new_tweets, lambda a,b : a['msg'] == b['msg'])

        if len(old_to_delete) > 0:
            for old in old_to_delete:
                old_tweets.remove(old)
                self.remove_tweet_by_text(old)

        # Add new tweets in the file to our tweet list
        new_to_add = elems_missing_in_other_list(new_tweets, old_tweets, lambda a,b : a['msg'] == b['msg'])
        if len(new_to_add) > 0:
            for new in new_to_add:
                self.tweets.append(new)

    def tweet_is_valid(self, tweet):
        """
        Checks if tweet is valid, measuring the length for every tweet and checking
        how much space will use each url in the tweet content.
        :param tweet:
        :return: True if tweet length will be correct. False otherwise
        """
        tweet_length = len(tweet)

        if tweet_length == 0:
            return False

        if tweet_length > TWEET_MAX_LENGHT:
            urls = re.findall('https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+([\w.,@?^=%&:/~+#-]*[\w@?^=%&/~+#-])?', tweet)
            if len(urls) == 0:
                return False

            urls_real_length = 0
            for url in urls:
                urls_real_length += len(url)
            tweet_length -= urls_real_length
            tweet_length += len(urls) * TWITTER_CONSIDERED_URL_LENGHT
            return tweet_length < TWEET_MAX_LENGHT
        return True

    def tweet_in_rotation(self, tweet):
        if self.max_seconds_in_rotation > 0 and tweet['first_pub_ts'] > 0:
            time_since_first_pub = int(time.time()) - tweet['first_pub_ts']
            if time_since_first_pub > self.max_seconds_in_rotation:
                print("tweet_id_due() OUT OF ROTATION max_seconds_in_rotation - " + tweet['msg'])
                return False
        return True

    def tweet_is_due(self, tweet):
        time_since_last_pub = int(time.time()) - tweet['last_pub_ts']
        is_due = time_since_last_pub > self.repetition_interval_in_seconds
        print("tweet_is_due(): " + str(is_due) + " time_since_last_pub=" + str(time_since_last_pub) + "s vs rep_interval=" + str(self.repetition_interval_in_seconds) + "s) - " + tweet['msg'])
        return is_due

    def file_was_modified(self):
        return self.file_feed_last_modified != os.stat(self.feed_path).st_mtime

    def process_tweets(self):
        print("Processing from FileFeed")
        try:
            self.tweets = FileFeed.load_tweets(self.feed_path, self.shuffle)
        except Exception as e:
            raise e

        index = 0
        while True:
            if not os.path.isfile(self.feed_path):
                raise IOError("%s no longer a valid file, aborting execution" % self.feed_path)

            if self.file_was_modified():
                print("Feed file was modified!, updating tweets")
                self.file_feed_last_modified = os.stat(self.feed_path).st_mtime
                index = 0
                self.update_tweets()
                continue

            tweet = self.tweets[index]

            if not self.tweet_in_rotation(tweet) or (tweet['last_pub_ts'] > 0 and not self.tweet_is_due(tweet)):
                if not self.tweet_in_rotation(tweet):
                   print("Tweet no longer in rotation (%s)" % tweet['msg'])
                elif tweet['last_pub_ts'] > 0 and not self.tweet_is_due(tweet):
                   print("It's too soon to publish this tweet again (%s)" % tweet['msg'])
                if index == len(self.tweets)-1:
                    print("\nsleeping (all tweets non-due, avoid cpu hogging)", self.intervals_between_tweets_in_seconds, "seconds... ZZzzz zzz")
                    if not self.dry_run:
                        time.sleep(self.intervals_between_tweets_in_seconds)
                index = inc_index(index, self.tweets)
                continue

            if not self.tweet_is_valid(tweet['msg']):
                print("The tweet located in ", self.feed_path, " line ", index+1 , " isn't valid.")
                index = inc_index(index, self.tweets)
                continue

            if self.dry_run:
                print("[DRY-RUN tweet! (", index, ")] tweet['msg'], )", tweet['msg'],"\n")
            else:
                try:
                    self.twitter_api.statuses.update(status=tweet['msg'])
                except TwitterHTTPError:
                    print("Skipping ", tweet['msg'], ' considered as a duplicate')

            tweet['last_pub_ts'] = int(time.time())
            self.tweets[index] = tweet # update the list with the modified dict

            index = inc_index(index, self.tweets)
            print("\nsleeping ", self.intervals_between_tweets_in_seconds, "seconds... ZZzzz zzz")
            if not self.dry_run:
                time.sleep(self.intervals_between_tweets_in_seconds)

    @staticmethod
    def load_tweets(feed_path, shuffle):
        tweets = []
        if not os.path.isfile(feed_path):
            print("%s is not a file" % feed_path)
            raise IOError("%s is not a file", feed_path)

        with open(feed_path, 'r') as tweets_file:
            tweets = [{'msg': t, 'last_pub_ts': 0, 'first_pub_ts':0} for t in tweets_file.readlines()]

        last_modification = os.stat(feed_path).st_mtime

        if len(tweets) == 0:
            raise IOError("%s file is empty ", feed_path)

        if shuffle:
            print("FileFeed.load_tweets: Randomizing tweets")
            random.shuffle(tweets)
            random.shuffle(tweets)

        return tweets

class URLFeed(Feed):
    """
    Reads titles from a wordpress RSS feed and uses them as tweets.

    Config values particular to an URL feed:
    'feed_url': The URL to the RSS feed
    'attached_featured_images': If set to True it will look in the target HTML for a featured image, download it and attach it to the tweet
    'num_last_tweets': How many of the last posts to tweet about, the rest are ignored
    'expirable_categories:'"<category_name_in_lower_case>":<max_seconds_in_rotation>,...'
    """
    def __init__(self, config, section):
        super(URLFeed, self).__init__(config, section)
        self.feed_url = config.get(section, 'feed_url')
        self.num_last_tweets = config.getint(section, 'num_last_tweets')
        self.attach_featured_images = config.getboolean(section, 'attach_featured_images')
        self.expirable_categories = None
        if config.has_option(section, 'expirable_categories'):
            self.expirable_categories = parse_expirable_categories(config.get(section, 'expirable_categories'), self.max_seconds_in_rotation)
        if self.dry_run:
            print("[DRY-RUN] URLFeed.expirable_categories = ", self.expirable_categories)

    def process_tweets(self):
        print("URLFeed.process_tweet: feedparser.parse(url=" + self.feed_url + ")")
        feed_dict = feedparser.parse(self.feed_url)
        print("URLFeed.process_tweets: got parsed feed_dict")

        num_tweets = len(feed_dict.entries)
        print("Got " + str(num_tweets) + " tweets from " + self.feed_url)
        if self.num_last_tweets != -1:
            num_tweets = min(self.num_last_tweets, len(feed_dict.entries))

        if self.dry_run and num_tweets is 0:
            print("[DRY-RUN] Got", num_tweets,"tweets")

        for i in range(num_tweets):
            entry = feed_dict.entries[i]
            title = entry.title
            if len(title) > MAX_TITLE_LENGTH:
                title = title[:MAX_TITLE_LENGTH]+'...'

            seconds_old = time.mktime(entry.published_parsed)
            now = time.time()

            if (now-seconds_old) > self.max_seconds_in_rotation:
                if self.dry_run:
                    print("[DRY-RUN (", i, ")] skipping (too old) ", title)
                    print()
                continue

            expirable_category_matched = self.get_matching_expirable_category(entry)
            if expirable_category_matched is not None:
                if (now-seconds_old) > expirable_category_matched.max_seconds_in_rotation:
                    if self.dry_run:
                        print("[DRY-RUN] (",i, ")] skipping (expirable category ", expirable_category_matched.category_name ," too old) ", title)
                        print()
                    continue
                elif self.dry_run:
                    print("[DRY-RUN] (",i, ")] has expirable category ", expirable_category_matched.category_name ," but still fresh")

            if self.attach_featured_images:
                featured_image_url = None
                if not self.dry_run:
                    featured_image_url = URLFeed.scrape_entry_og_image(entry)
                else:
                    print("[DRY-RUN] scrape and download og:image for entry " + entry.link)

                if featured_image_url is not None:
                    if not self.dry_run:
                        img_data = download_image(featured_image_url)
                        if img_data is not None:
                          img_id = self.twitter_upload_api.media.upload(media=img_data)["media_id_string"]
                          self.twitter_api.statuses.update(status=title + " " + entry.link, media_ids = img_id) # Call twitter API
                        else:
                          self.twitter_api.statuses.update(status=title + " " + entry.link)
                    else:
                        print("[DRY-RUN (", i, ")] ", title + " " + entry.link, " (+img attachment - ", featured_image_url, ")")
                        print("[DRY-RUN (", i, ")] time() -> ", time.mktime(entry.published_parsed))
                        img_data = download_image(featured_image_url)
                        print("[DRY-RUN (", i, ")] ", len(img_data))
                else:
                    print("Could not scrape featured image url from ", entry.link, " (regex might need maintenance)")
            else:
                if not self.dry_run:
                    try:
                        self.twitter_api.statuses.update(status=title + " " + entry.link) # Call twitter API
                    except TwitterHTTPError as e:
                        print("TwitterHTTPError", e)
                        print("Could not tweet status=" + title)
                        continue
                else:
                    print("[DRY-RUN (", i, ")] ", title + " " + entry.link)

            if self.intervals_between_tweets_in_seconds > 0:
                if not self.dry_run:
                    time.sleep(self.intervals_between_tweets_in_seconds)
                else:
                    print("[DRY-RUN (", i, ")] ", "sleep(", self.intervals_between_tweets_in_seconds,")\n")

    @staticmethod
    def fetch_html(url):
        f = urllib.request.urlopen(url)
        html = f.read()
        f.close()
        return html

    @staticmethod
    def scrape_entry_og_image(entry):
        """
        Download a feed's entry HTML and parse it to find the og:image in the entry if available.

        :param entry:
        :return: the url of the featured image if found, otherwise None
        """
        featured_image_url = None
        html = URLFeed.fetch_html(entry.link)
        match = og_image_pattern.search(html.decode('utf-8'))
        if match is not None:
            featured_image_url = match.group(1)
        return featured_image_url

    @staticmethod
    def scrape_entry_categories(entry):
        """
        Returns a string list with the names of the categories of this post
        soup.find_all(name="a",attrs={'rel':'category'})[0].attrs['href']
        """
        html_doc = URLFeed.fetch_html(entry.link)
        soup = BeautifulSoup(html_doc,'html.parser')
        soup_cats = soup.find_all(name="a",attrs={'rel':'category'})
        categories_names = []
        for soup_cat in soup_cats:
            href = soup_cat.attrs['href']
            #print("scrape_entry_categories: href=" + href)
            cat_start = -1
            if href.find('/category/') >= 0:
                cat_start = href.find('/category/') + 10
            elif href.find('/categoria/') >= 0:
                cat_start = href.find('/categoria/') + 11

            if cat_start > -1:
                #print("scrape_entry_categories: cat_start=" + str(cat_start))
                #print("scrape_entry_categories: adding category: [" + href[cat_start:-1] + "]")
                categories_names.append(href[cat_start:-1])
            if len(categories_names) >= 5:
                break
        return categories_names

    def get_matching_expirable_category(self, entry):
        if self.expirable_categories is None:
            return None
        entry_categories = URLFeed.scrape_entry_categories(entry)
        if self.dry_run:
            print("[DRY-RUN] entry_categories")
            print(entry_categories)
        for expirable_category in self.expirable_categories:
            for entry_category_str in entry_categories:
                if self.dry_run:
                    print("[DRY-RUN] comparing entry:" + entry_category_str + " vs expirable:" + expirable_category.category_name)
                if entry_category_str == expirable_category.category_name:
                    return expirable_category
        return None

class ExpirableCategory:
    def __init__(self, category_name_str, max_seconds_in_rotation_str, default_max_rotation_secs):
        self.category_name = category_name_str
        try:
          self.max_seconds_in_rotation = int(max_seconds_in_rotation_str)
        except:
          self.max_seconds_in_rotation = default_max_rotation_secs


def parse_expirable_categories(config_expirable_categories_string, default_max_rotation_secs):
    """"<category_name_in_lower_case>":<max_seconds_in_rotation>,..."""
    expirable_categories = []
    expirable_category_string_list = config_expirable_categories_string.split(',')
    for expirable_category_string in expirable_category_string_list:
        try:
            cat_expiration_list = expirable_category_string.split(':')
            if len(cat_expiration_list) != 2:
                continue
            expirable_categories.append(ExpirableCategory(cat_expiration_list[0], cat_expiration_list[1], default_max_rotation_secs))
        except:
            continue
    return expirable_categories

def download_image(img_url):
    file_path = "image.tmp"
    try:
      urllib.request.urlretrieve(img_url, file_path)
      f = open(file_path,'rb')
      img_data = f.read()
      f.close()
      return img_data
    except:
      return None

def inc_index(index, collection):
    return (index+1) % len(collection)

def elems_missing_in_other_list(the_one, the_other, comparison_function):
    missing = []
    for one in the_one:
        one_found = False
        for other in the_other:
            if comparison_function(one, other):
                one_found = True
                break
        if not one_found:
            missing.append(one)
    return missing

def load_config(configFile='config.conf'):
    """
    Returns a configparser object that contains all the configuration for the different blogs and twitter accounts.
    :param configFile:
    :return:
    """
    config = configparser.ConfigParser()
    config.read(configFile)
    return config


if __name__ == '__main__':
    if len(sys.argv) < 2:
        print("Error: config file path not passed.\n")
        sys.exit(1)
    if hasattr(ssl, '_create_unverified_context'):
        ssl._create_default_https_context = ssl._create_unverified_context
        print("ssl: create unverified context as a default https context")
    config = load_config(sys.argv[1])
    for section_name in config.sections():
        if config.has_option(section_name, 'feed_url'):
            feed = URLFeed(config, section_name)
        elif config.has_option(section_name, 'feed_path'):
            feed = FileFeed(config, section_name)

        thread = threading.Thread(target=feed.process_tweets)
        thread.daemon = False
        thread.start()
