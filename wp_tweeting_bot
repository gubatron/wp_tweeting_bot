#!/usr/bin/env python
import os
import configparser
import feedparser
import urllib
import re
import time
import sys
import threading
from twitter import Twitter, OAuth, TwitterHTTPError

TWITTER_CONSIDERED_URL_LENGHT = 23
TWEET_MAX_LENGHT = 280
og_image_pattern = re.compile('.*og:image" content="(.*?)".*?\/>')


class Feed(object):
    """
    Base class for feeds.
    Contains all the logic to do authentication on twitter service.
    Each class that inherits must implements process_tweets on his own.
    dry_run method is optional. Just needed to do testing.
    """
    def __init__(self, config, section):
        self.oauth_token = config.get(section, 'oauth_token')
        self.oauth_token_secret = config.get(section, 'oauth_token_secret')
        self.oauth_consumer_key = config.get(section, 'oauth_consumer_key')
        self.oauth_consumer_secret = config.get(section, 'oauth_consumer_secret')
        self.dry_run = config.getboolean(section, 'dry_run')
        self.intervals_between_tweets_in_seconds = config.getint(section, 'intervals_between_tweets_in_seconds')
        self.max_days_in_rotation = config.getint(section, 'max_days_in_rotation')
        self.max_seconds_in_rotation = -1
        if self.max_days_in_rotation != -1:
            self.max_seconds_in_rotation = self.max_days_in_rotation * 24 * 60 * 60

        # Auth Credentials
        self.oauth = OAuth(self.oauth_token, self.oauth_token_secret, self.oauth_consumer_key, self.oauth_consumer_secret)
        self.twitter_api = Twitter(auth=self.oauth)
        self.twitter_upload_api = Twitter(auth=self.oauth, domain='upload.twitter.com')

    def process_tweets(self):
        raise NotImplementedError

    def __str__(self):
        return str(self.__dict__)

class FileFeed(Feed):
    """
    A file feed reads one tweet per line from a text file, tweets are rotated according to the configuration settings.

    Special configuration options for FileFeed:
       'feed_path': full file path of text file containing tweets
       'repetition_interval_in_seconds': how many seconds to wait before repeating a tweet

    Internally tweets are kept as a list of dictionaries
    [{'msg': <the tweet text>,
      'first_pub_ts': <timestamp of the first time this message was tweeted>,
      'last_pub_ts': <timestamp of the last time this message was tweeted> }]
    """
    def __init__(self, config, section):
        super(FileFeed, self).__init__(config, section)
        self.feed_path = config.get(section, 'feed_path')
        self.tweets = []
        self.file_feed_last_modified = os.stat(self.feed_path).st_mtime
        self.repetition_interval_in_seconds = config.getint(section, 'repetition_interval_in_seconds')

    def remove_tweet_by_text(self, old_tweet):
        for t in self.tweets:
            if t['msg'] == old_tweet['msg']:
                self.tweets.remove(t)
                break

    def update_tweets(self):
        old_tweets = self.tweets[:]
        new_tweets = FileFeed.load_tweets(self.feed_path)

        # Delete old tweets that are no longer in the file
        old_to_delete = elems_missing_in_other_list(old_tweets, new_tweets, lambda a,b : a['msg'] == b['msg'])

        if len(old_to_delete) > 0:
            for old in old_to_delete:
                old_tweets.remove(old)
                self.remove_tweet_by_text(old)

        # Add new tweets in the file to our tweet list
        new_to_add = elems_missing_in_other_list(new_tweets, old_tweets, lambda a,b : a['msg'] == b['msg'])
        if len(new_to_add) > 0:
            for new in new_to_add:
                self.tweets.append(new)

    def tweet_is_valid(self, tweet):
        """
        Checks if tweet is valid, measuring the length for every tweet and checking
        how much space will use each url in the tweet content.
        :param tweet:
        :return: True if tweet length will be correct. False otherwise
        """
        tweet_length = len(tweet)

        if tweet_length == 0:
            return False

        if tweet_length > TWEET_MAX_LENGHT:
            urls = re.findall('https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+([\w.,@?^=%&:/~+#-]*[\w@?^=%&/~+#-])?', tweet)
            if len(urls) == 0:
                return False

            urls_real_length = 0
            for url in urls:
                urls_real_length += len(url)
            tweet_length -= urls_real_length
            tweet_length += len(urls) * TWITTER_CONSIDERED_URL_LENGHT
            return tweet_length < TWEET_MAX_LENGHT
        return True

    def tweet_in_rotation(self, tweet):
        if self.max_seconds_in_rotation > 0 and tweet['first_pub_ts'] > 0:
            time_since_first_pub = int(time.time()) - tweet['first_pub_ts']
            if time_since_first_pub > self.max_seconds_in_rotation:
                print "tweet_id_due() OUT OF ROTATION max_seconds_in_rotation - " + tweet['msg']
                return False
        return True

    def tweet_is_due(self, tweet):
        time_since_last_pub = int(time.time()) - tweet['last_pub_ts']
        is_due = time_since_last_pub > self.repetition_interval_in_seconds
        print "tweet_is_due(): " + str(is_due) + " time_since_last_pub=" + str(time_since_last_pub) + "s vs rep_interval=" + str(self.repetition_interval_in_seconds) + "s) - " + tweet['msg']
        return is_due

    def file_was_modified(self):
        return self.file_feed_last_modified != os.stat(self.feed_path).st_mtime

    def process_tweets(self):
        print("Processing from FileFeed")
        try:
            self.tweets = FileFeed.load_tweets(self.feed_path)
        except Exception as e:
            raise e

        index = 0
        while True:
            if not os.path.isfile(self.feed_path):
                raise IOError("%s no longer a valid file, aborting execution" % self.feed_path)

            if self.file_was_modified():
                print "Feed file was modified!, updating tweets"
                self.file_feed_last_modified = os.stat(self.feed_path).st_mtime
                index = 0
                self.update_tweets()
                continue

            tweet = self.tweets[index]

            if not self.tweet_in_rotation(tweet) or (tweet['last_pub_ts'] > 0 and not self.tweet_is_due(tweet)):
                if not self.tweet_in_rotation(tweet):
                   print "Tweet no longer in rotation (%s)" % tweet['msg']
                elif tweet['last_pub_ts'] > 0 and not self.tweet_is_due(tweet):
                   print "It's too soon to publish this tweet again (%s)" % tweet['msg']                
                if index == len(self.tweets)-1:
                    print "\nsleeping (all tweets non-due, avoid cpu hogging)", self.intervals_between_tweets_in_seconds, "seconds... ZZzzz zzz"
                    time.sleep(self.intervals_between_tweets_in_seconds)
                index = inc_index(index, self.tweets)
                continue

            if not self.tweet_is_valid(tweet['msg']):
                print "The tweet located in ", self.feed_path, " line ", index+1 , " isn't valid."
                index = inc_index(index, self.tweets)
                continue

            if self.dry_run:
                print "[DRY-RUN tweet! (", index, ")] tweet['msg'], )", tweet['msg'],"\n"
            else:
                try:
                    self.twitter_api.statuses.update(status=tweet['msg'])
                except TwitterHTTPError:
                    print "Skipping ", tweet['msg'], ' considered as a duplicate'

            tweet['last_pub_ts'] = int(time.time())
            self.tweets[index] = tweet # update the list with the modified dict

            index = inc_index(index, self.tweets)
            print "\nsleeping ", self.intervals_between_tweets_in_seconds, "seconds... ZZzzz zzz"
            time.sleep(self.intervals_between_tweets_in_seconds)

    @staticmethod
    def load_tweets(feed_path):
        tweets = []
        if not os.path.isfile(feed_path):
            print "%s is not a file" % feed_path
            raise IOError("%s is not a file", feed_path)

        with open(feed_path, 'r') as tweets_file:
            tweets = [{'msg': t, 'last_pub_ts': 0, 'first_pub_ts':0} for t in tweets_file.readlines()]

        last_modification = os.stat(feed_path).st_mtime

        if len(tweets) == 0:
            raise IOError("%s file is empty ", feed_path)
        return tweets

class URLFeed(Feed):
    """
    Reads titles from a wordpress RSS feed and uses them as tweets.
    
    Config values particular to an URL feed:
    'feed_url': The URL to the RSS feed
    'attached_featured_images': If set to True it will look in the target HTML for a featured image, download it and attach it to the tweet
    'num_last_tweets': How many of the last posts to tweet about, the rest are ignored
    """
    def __init__(self, config, section):
        super(URLFeed, self).__init__(config, section)
        self.feed_url = config.get(section, 'feed_url')
        self.num_last_tweets = config.getint(section, 'num_last_tweets')
        self.attach_featured_images = config.getboolean(section, 'attach_featured_images')

    def process_tweets(self):
        feed_dict = feedparser.parse(self.feed_url)

        num_tweets = len(feed_dict.entries)
        if self.num_last_tweets != -1:
            num_tweets = min(self.num_last_tweets, len(feed_dict.entries))

        if self.dry_run and num_tweets is 0:
            print "[DRY-RUN] Got", num_tweets,"tweets"
            print feed_dict.headers
            print feed_dict
            print self
                
        for i in xrange(num_tweets):
            entry = feed_dict.entries[i]
            title = entry.title
            if len(title) > 231:
                title = title[:231]+'...'

            seconds_old = time.mktime(entry.published_parsed)
            now = time.time()
            if (now-seconds_old) > self.max_seconds_in_rotation:
                if self.dry_run:
                    print "[DRY-RUN (", i, ")] skipping (too old) ", title
                continue
                            
            if self.attach_featured_images:
                featured_image_url = URLFeed.scrape_entry_og_image(entry)

                if featured_image_url is not None:
                    if not self.dry_run:
                        img_data = download_image(featured_image_url)
                        img_id = self.twitter_upload_api.media.upload(media=img_data)["media_id_string"]
                        self.twitter_api.statuses.update(status=title + " " + entry.link, media_ids = img_id) # Call twitter API
                    else:
                        print "[DRY-RUN (", i, ")] ", title + " " + entry.link, " (+img attachment - ", featured_image_url, ")"
                        print "[DRY-RUN (", i, ")] time() -> ", time.mktime(entry.published_parsed)
                        img_data = download_image(featured_image_url)
                        print "[DRY-RUN (", i, ")] ", len(img_data)
                else:
                    print "Could not scrape featured image url from ", entry.link, " (regex might need maintenance)"
            else:
                if not self.dry_run:
                    self.twitter_api.statuses.update(status=title + " " + entry.link) # Call twitter API
                else:
                    print "[DRY-RUN (", i, ")] ", title + " " + entry.link

            if self.intervals_between_tweets_in_seconds > 0:
                if not self.dry_run:
                    time.sleep(self.intervals_between_tweets_in_seconds)
                else:
                    print "[DRY-RUN (", i, ")] ", "sleep(", self.intervals_between_tweets_in_seconds,")\n"

    @staticmethod
    def scrape_entry_og_image(entry):
        """
        Download a feed's entry HTML and parse it to find the og:image in the entry if available.

        :param entry:
        :return: the url of the featured image if found, otherwise None
        """
        featured_image_url = None
        f = urllib.urlopen(entry.link)
        html = f.read()
        f.close()
        match = og_image_pattern.search(html)
        if match is not None:
            featured_image_url = match.group(1)
        return featured_image_url

def download_image(img_url):
    file_path = "image.tmp"
    urllib.urlretrieve(img_url, file_path)
    f = open(file_path,'rb')
    img_data = f.read()
    f.close()
    return img_data

def inc_index(index, collection):
    return (index+1) % len(collection)

def elems_missing_in_other_list(the_one, the_other, comparison_function):
    missing = []
    for one in the_one:
        one_found = False
        for other in the_other:
            if comparison_function(one, other):
                one_found = True
                break
        if not one_found:
            missing.append(one)
    return missing

def load_config(configFile='config.conf'):
    """
    Returns a configparser object that contains all the configuration for the different blogs and twitter accounts.
    :param configFile:
    :return:
    """
    config = configparser.ConfigParser()
    config.read(configFile)
    return config


if __name__ == '__main__':
    if len(sys.argv) < 2:
        print "Error: config file path not passed.\n"
        sys.exit(1)
        
    config = load_config(sys.argv[1])
    for section_name in config.sections():
        try:
            feed = URLFeed(config, section_name)
        except configparser.NoOptionError:
            feed = FileFeed(config, section_name)

        thread = threading.Thread(target=feed.process_tweets)
        thread.daemon = False
        thread.start()
